{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPhf2AZjCDvldGZMebMjp/w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thepersonuadmire/KNNandPCA/blob/main/KNN_%26_PCA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Theoretical"
      ],
      "metadata": {
        "id": "pKfMo9W48td2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is K-Nearest Neighbors (KNN) and how does it work?\n"
      ],
      "metadata": {
        "id": "t0kzbzkG8vBu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "K-Nearest Neighbors (KNN) is a non-parametric, instance-based learning algorithm used for classification and regression. It works by finding the 'K' closest training examples in the feature space to a given test instance and making predictions based on the majority class (for classification) or the average (for regression) of those neighbors."
      ],
      "metadata": {
        "id": "cJl9uaWxAKlo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. What is the difference between KNN Classification and KNN Regression?\n"
      ],
      "metadata": {
        "id": "5ROZlCbg9V69"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In KNN Classification, the algorithm predicts the class label of a data point based on the majority class of its K nearest neighbors. In KNN Regression, it predicts a continuous value by averaging the values of the K nearest neighbors."
      ],
      "metadata": {
        "id": "fCu3fF6MALyK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the role of the distance metric in KNN?\n"
      ],
      "metadata": {
        "id": "oR1FnrSP9Xvy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distance metric (e.g., Euclidean, Manhattan) determines how the distance between data points is calculated. It influences the selection of neighbors and, consequently, the predictions made by the KNN algorithm."
      ],
      "metadata": {
        "id": "BiGvoUeaAP7I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the Curse of Dimensionality in KNN?\n"
      ],
      "metadata": {
        "id": "-tPjtKLk9Zzg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Curse of Dimensionality refers to the phenomenon where the feature space becomes increasingly sparse as the number of dimensions increases. This sparsity makes it difficult for KNN to find meaningful neighbors, leading to poor performance in high-dimensional spaces."
      ],
      "metadata": {
        "id": "ppSmYlpyASz_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How can we choose the best value of K in KNN?\n"
      ],
      "metadata": {
        "id": "kCe9_IVa9bmq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The best value of K can be chosen using techniques like cross-validation. Typically, a smaller K can lead to a more complex model (overfitting), while a larger K can smooth out the decision boundary (underfitting). A common approach is to test multiple values of K and select the one that yields the best validation accuracy."
      ],
      "metadata": {
        "id": "7eh2G7rCAVjV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What are KD Tree and Ball Tree in KNN?\n"
      ],
      "metadata": {
        "id": "5ldGIlIF9dlk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "KD Tree and Ball Tree are data structures used to organize points in a k-dimensional space to enable efficient nearest neighbor searches. KD Tree partitions the space into hyperplanes, while Ball Tree uses hyperspheres to group points."
      ],
      "metadata": {
        "id": "HDAIqk08AY9a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. When should you use KD Tree vs. Ball Tree?\n"
      ],
      "metadata": {
        "id": "z3wfpYdo9fRq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "KD Tree is generally more efficient for low-dimensional data, while Ball Tree can be more effective for high-dimensional data due to its ability to handle non-uniform distributions better."
      ],
      "metadata": {
        "id": "CcIjR2a9AcB4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What are the disadvantages of KNN?\n"
      ],
      "metadata": {
        "id": "QM_mss229g8M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Disadvantages of KNN include high computational cost during prediction (as it requires calculating distances to all training samples), sensitivity to irrelevant features, and poor performance in high-dimensional spaces due to the Curse of Dimensionality."
      ],
      "metadata": {
        "id": "rcsunPQ4AeuI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. How does feature scaling affect KNN?\n"
      ],
      "metadata": {
        "id": "bg9ajBJ39ify"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature scaling is crucial for KNN because the algorithm relies on distance calculations. If features are on different scales, those with larger ranges can disproportionately influence the distance metric, leading to biased predictions."
      ],
      "metadata": {
        "id": "3Eatr98sAhiO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What is PCA (Principal Component Analysis)?\n"
      ],
      "metadata": {
        "id": "LhN-zOYB9kQm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA is a dimensionality reduction technique that transforms a dataset into a set of orthogonal (uncorrelated) components, capturing the maximum variance in the data with fewer dimensions."
      ],
      "metadata": {
        "id": "O26POevNAlKo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. How does PCA work?\n"
      ],
      "metadata": {
        "id": "y8u3q8bi9mK5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA works by computing the covariance matrix of the data, finding its eigenvalues and eigenvectors, and then projecting the data onto the eigenvectors corresponding to the largest eigenvalues."
      ],
      "metadata": {
        "id": "gLMLld9cAoY8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is the geometric intuition behind PCA?\n"
      ],
      "metadata": {
        "id": "eO0YGrbL9yls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Geometrically, PCA identifies the directions (principal components) in which the data varies the most and projects the data onto these directions, effectively reducing dimensionality while preserving variance."
      ],
      "metadata": {
        "id": "bptP1n7PArCZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What is the difference between Feature Selection and Feature Extraction?\n"
      ],
      "metadata": {
        "id": "9wTzZen-90NB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Selection involves selecting a subset of the original features based on their importance, while Feature Extraction creates new features from the original ones (e.g., PCA)."
      ],
      "metadata": {
        "id": "0ZXE-MZUAuTw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What are Eigenvalues and Eigenvectors in PCA?\n"
      ],
      "metadata": {
        "id": "7Mf_LtM591y-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Eigenvalues represent the amount of variance captured by each principal component, while eigenvectors indicate the direction of these components in the feature space."
      ],
      "metadata": {
        "id": "rdfhT9SVA1Ge"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. How do you decide the number of components to keep in PCA?\n"
      ],
      "metadata": {
        "id": "xO09_tqf93kO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of components can be decided by examining the explained variance ratio and choosing a threshold (e.g., 95% of total variance) or using a scree plot to identify the \"elbow\" point."
      ],
      "metadata": {
        "id": "3MLXRmeRBAhS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. Can PCA be used for classification?\n"
      ],
      "metadata": {
        "id": "7Oto6VW097rY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA itself is not a classification method, but it can be used as a preprocessing step to reduce dimensionality before applying classification algorithms."
      ],
      "metadata": {
        "id": "4WnQGn_BBGXO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What are the limitations of PCA?\n"
      ],
      "metadata": {
        "id": "s8DCSCca9-6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Limitations of PCA include its sensitivity to outliers, the assumption of linearity, and the fact that it may not capture complex relationships in the data."
      ],
      "metadata": {
        "id": "ie61qw43BJRC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. How do KNN and PCA complement each other?\n"
      ],
      "metadata": {
        "id": "6ycAKh0y-A4o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA can reduce the dimensionality of the dataset, making KNN more efficient and potentially improving its performance by removing noise and irrelevant features."
      ],
      "metadata": {
        "id": "O84vgE_ZBMAL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How does KNN handle missing values in a dataset?\n"
      ],
      "metadata": {
        "id": "x-tvDioS-ClY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN does not inherently handle missing values. However, techniques like KNN imputation can be used to estimate missing values based on the nearest neighbors."
      ],
      "metadata": {
        "id": "9v-tyaJSBPdV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. What are the key differences between PCA and Linear Discriminant Analysis (LDA)?"
      ],
      "metadata": {
        "id": "93OvRskK-EYC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA is an unsupervised method that focuses on maximizing variance, while LDA is a supervised method that aims to maximize class separability. LDA uses class labels to find the optimal projection, whereas PCA does not."
      ],
      "metadata": {
        "id": "UHU5QrYgBSOf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Practical"
      ],
      "metadata": {
        "id": "0Eri0qCZ8aPK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. Train a KNN Classifier on the Iris dataset and print model accuracy.\n"
      ],
      "metadata": {
        "id": "QHC844qa8eXt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(X_train, y_train)\n",
        "accuracy = knn.score(X_test, y_test)\n",
        "print(f'Model Accuracy: {accuracy:.2f}')\n"
      ],
      "metadata": {
        "id": "nWSyActnBWLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "22. Train a KNN Regressor on a synthetic dataset and evaluate using Mean Squared Error (MSE).\n"
      ],
      "metadata": {
        "id": "nz8qFah5_E4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X, y = make_regression(n_samples=100, n_features=1, noise=0.1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "knn_regressor = KNeighborsRegressor(n_neighbors=5)\n",
        "knn_regressor.fit(X_train, y_train)\n",
        "y_pred = knn_regressor.predict(X_test)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(f'Mean Squared Error: {mse:.2f}')"
      ],
      "metadata": {
        "id": "ZwCx74NFBYan"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "23. Train a KNN Classifier using different distance metrics (Euclidean and Manhattan) and compare accuracy.\n"
      ],
      "metadata": {
        "id": "NJn3M5SH_HLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=3, metric='euclidean')\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=3, metric='manhattan')\n",
        "knn_euclidean.fit(X_train, y_train)\n",
        "knn_manhattan.fit(X_train, y_train)\n",
        "accuracy_euclidean = knn_euclidean.score(X_test, y_test)\n",
        "accuracy_manhattan = knn_manhattan.score(X_test, y_test)\n",
        "print(f'Accuracy (Euclidean): {accuracy_euclidean:.2f}')\n",
        "print(f'Accuracy (Manhattan): {accuracy_manhattan:.2f}')\n"
      ],
      "metadata": {
        "id": "a1WKiuN4Bkdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "24. Train a KNN Classifier with different values of K and visualize decision boundaries.\n"
      ],
      "metadata": {
        "id": "wmRyqSXD_JRs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data[:, :2]  # Use only the first two features for visualization\n",
        "y = iris.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a mesh grid for plotting\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.01), np.arange(y_min, y_max, 0.01))\n",
        "\n",
        "for k in [1, 3, 5]:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_train, y_train)\n",
        "    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.contourf(xx, yy, Z, alpha=0.3)\n",
        "    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolor='k', marker='o', label='Train')\n",
        "    plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, edgecolor='k', marker='x', label='Test')\n",
        "    plt.title(f'Decision Boundary for K={k}')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        ""
      ],
      "metadata": {
        "id": "gkVd8QLmBnD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "25. Apply Feature Scaling before training a KNN model and compare results with unscaled data.\n"
      ],
      "metadata": {
        "id": "1SFL8ntP_LP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# 1. Train KNN Classifier without scaling\n",
        "knn_unscaled = KNeighborsClassifier(n_neighbors=3)\n",
        "knn_unscaled.fit(X_train, y_train)\n",
        "accuracy_unscaled = knn_unscaled.score(X_test, y_test)\n",
        "\n",
        "# 2. Apply feature scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# 3. Train KNN Classifier with scaling\n",
        "knn_scaled = KNeighborsClassifier(n_neighbors=3)\n",
        "knn_scaled.fit(X_train_scaled, y_train)\n",
        "accuracy_scaled = knn_scaled.score(X_test_scaled, y_test)\n",
        "\n",
        "# Print the results\n",
        "print(f'Accuracy without scaling: {accuracy_unscaled:.2f}')\n",
        "print(f'Accuracy with scaling: {accuracy_scaled:.2f}')"
      ],
      "metadata": {
        "id": "S-gNuEArCDXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "26. Train a PCA model on synthetic data and print the explained variance ratio for each component.\n"
      ],
      "metadata": {
        "id": "GgZVi3Hi_N4c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "X, _ = make_classification(n_samples=100, n_features=10, n_informative=5, random_state=42)\n",
        "pca = PCA()\n",
        "pca.fit(X)\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "print(f'Explained Variance Ratio: {explained_variance_ratio}')\n"
      ],
      "metadata": {
        "id": "FMmEp-hyBsPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "27. Apply PCA before training a KNN Classifier and compare accuracy with and without PCA.\n"
      ],
      "metadata": {
        "id": "Y7AEWZqm_Pvs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Without PCA\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(X_train, y_train)\n",
        "accuracy_no_pca = knn.score(X_test, y_test)\n",
        "\n",
        "# With PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "knn.fit(X_train_pca, y_train)\n",
        "accuracy_with_pca = knn.score(X_test_pca, y_test)\n",
        "\n",
        "print(f'Accuracy without PCA: {accuracy_no_pca:.2f}')\n",
        "print(f'Accuracy with PCA: {accuracy_with_pca:.2f}')\n"
      ],
      "metadata": {
        "id": "kI9IPLwyCFDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "28. Perform Hyperparameter Tuning on a KNN Classifier using GridSearchCV.\n"
      ],
      "metadata": {
        "id": "ppFIgMWX_Xjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
        "param_grid = {'n_neighbors': [1, 3, 5, 7, 9], 'metric': ['euclidean', 'manhattan']}\n",
        "grid_search = GridSearchCV(KNeighborsClassifier(), param_grid, cv=5)\n",
        "grid_search.fit(X_train, y_train)\n",
        "best_params = grid_search.best_params_\n",
        "print(f'Best parameters: {best_params}')\n"
      ],
      "metadata": {
        "id": "c4hJyCvMCKwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "29. Train a KNN Classifier and check the number of misclassified samples.\n"
      ],
      "metadata": {
        "id": "naVg9NCh_aRG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred = knn.predict(X_test)\n",
        "misclassified_samples = (y_test != y_pred).sum()\n",
        "print(f'Number of misclassified samples: {misclassified_samples}')\n"
      ],
      "metadata": {
        "id": "xHZyAzC4CNGn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "30. Train a PCA model and visualize the cumulative explained variance.\n"
      ],
      "metadata": {
        "id": "PvZWx_DO_cng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "iris = load_iris()\n",
        "pca = PCA()\n",
        "pca.fit(iris.data)\n",
        "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(cumulative_variance, marker='o')\n",
        "plt.title('Cumulative Explained Variance')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.grid()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "8OuEe62PCPTv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "31. Train a KNN Classifier using different values of the weights parameter (uniform vs. distance) and compare accuracy.\n"
      ],
      "metadata": {
        "id": "EVUsQKvq_eh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
        "knn_uniform = KNeighborsClassifier(n_neighbors=3, weights='uniform')\n",
        "knn_distance = KNeighborsClassifier(n_neighbors=3, weights='distance')\n",
        "knn_uniform.fit(X_train, y_train)\n",
        "knn_distance.fit(X_train, y_train)\n",
        "accuracy_uniform = knn_uniform.score(X_test, y_test)\n",
        "accuracy_distance = knn_distance.score(X_test, y_test)\n",
        "print(f'Accuracy (Uniform Weights): {accuracy_uniform:.2f}')\n",
        "print(f'Accuracy (Distance Weights): {accuracy_distance:.2f}')\n"
      ],
      "metadata": {
        "id": "ybdlLgc6CR01"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "32. Train a KNN Regressor and analyze the effect of different K values on performance.\n"
      ],
      "metadata": {
        "id": "ZolRl5-Z_hEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X, y = make_regression(n_samples=100, n_features=1, noise=0.1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "k_values = [1, 3, 5, 7, 9]\n",
        "for k in k_values:\n",
        "    knn_regressor = KNeighborsRegressor(n_neighbors=k)\n",
        "    knn_regressor.fit(X_train, y_train)\n",
        "    y_pred = knn_regressor.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f'Mean Squared Error for K={k}: {mse:.2f}')\n"
      ],
      "metadata": {
        "id": "5oeCWoklCUyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "33. Implement KNN Imputation for handling missing values in a dataset.\n"
      ],
      "metadata": {
        "id": "ie7aMtzW_jH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.impute import KNNImputer\n",
        "import numpy as np\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "# Introduce missing values\n",
        "X[::10] = np.nan\n",
        "imputer = KNNImputer(n_neighbors=3)\n",
        "X_imputed = imputer.fit_transform(X)\n",
        "print(f'Imputed Data:\\n{X_imputed}')\n"
      ],
      "metadata": {
        "id": "PQnCwPJFCXAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "34. Train a PCA model and visualize the data projection onto the first two principal components.\n"
      ],
      "metadata": {
        "id": "jSLhWjNa_lcG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "iris = load_iris()\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(iris.data)\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=iris.target, edgecolor='k', cmap='viridis')\n",
        "plt.title('PCA Projection of Iris Dataset')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "WIIvt4tsCbcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "35. Train a KNN Classifier using the KD Tree and Ball Tree algorithms and compare performance.\n"
      ],
      "metadata": {
        "id": "YEI3ZDoH_nYk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "iris = load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.2, random_state=42)\n",
        "knn_kd_tree = KNeighborsClassifier(n_neighbors=3, algorithm='kd_tree')\n",
        "knn_ball_tree = KNeighborsClassifier(n_neighbors=3, algorithm='ball_tree')\n",
        "knn_kd_tree.fit(X_train, y_train)\n",
        "knn_ball_tree.fit(X_train, y_train)\n",
        "accuracy_kd_tree = knn_kd_tree.score(X_test, y_test)\n",
        "accuracy_ball_tree = knn_ball_tree.score(X_test, y_test)\n",
        "print(f'Accuracy (KD Tree): {accuracy_kd_tree:.2f}')\n",
        "print(f'Accuracy (Ball Tree): {accuracy_ball_tree:.2f}')\n"
      ],
      "metadata": {
        "id": "gDJx4SmvCdAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "36. Train a PCA model on a high-dimensional dataset and visualize the Scree plot.\n"
      ],
      "metadata": {
        "id": "rrO9taDS_pCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_classification\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "X, _ = make_classification(n_samples=100, n_features=20, n_informative=10, random_state=42)\n",
        "pca = PCA()\n",
        "pca.fit(X)\n",
        "plt.plot(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_, marker='o')\n",
        "plt.title('Scree Plot')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Variance Explained')\n",
        "plt.grid()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dWcwZD5aCfSR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "37. Train a KNN Classifier and evaluate performance using Precision, Recall, and F1-Score.\n"
      ],
      "metadata": {
        "id": "quN2mKz9_try"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "knn = KNeighborsClassifier()\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "wu5gO_CMC-vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "38. Train a PCA model and analyze the effect of different numbers of components on accuracy.\n"
      ],
      "metadata": {
        "id": "aSgbHCUi_vzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracies = []\n",
        "for n in range(1, X.shape[1]+1):\n",
        "    pca = PCA(n_components=n)\n",
        "    X_pca = pca.fit_transform(X)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_pca, y, random_state=42)\n",
        "    knn.fit(X_train, y_train)\n",
        "    acc = knn.score(X_test, y_test)\n",
        "    accuracies.append(acc)\n",
        "\n",
        "plt.plot(range(1, X.shape[1]+1), accuracies)\n",
        "plt.xlabel(\"Number of PCA Components\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy vs PCA Components\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oeHwyJxHDACU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "39. Train a KNN Classifier with different leaf_size values and compare accuracy.\n"
      ],
      "metadata": {
        "id": "gM_q2Q-l_xon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "leaf_sizes = [5, 10, 20, 30, 50]\n",
        "for leaf in leaf_sizes:\n",
        "    knn = KNeighborsClassifier(leaf_size=leaf)\n",
        "    knn.fit(X_train, y_train)\n",
        "    print(f\"Leaf Size: {leaf}, Accuracy: {knn.score(X_test, y_test)}\")\n"
      ],
      "metadata": {
        "id": "UNKeNGINDE09"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "40. Train a PCA model and visualize how data points are transformed before and after PCA.\n"
      ],
      "metadata": {
        "id": "jPUQDQrN_zhk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
        "plt.title(\"Original Data\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y)\n",
        "plt.title(\"After PCA (2D)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "uURlmBmGDQ4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "41. Train a KNN Classifier on a real-world dataset (Wine dataset) and print classification report.\n"
      ],
      "metadata": {
        "id": "Fk9hcN6Z_1Gy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_wine()\n",
        "X, y = data.data, data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "knn.fit(X_train, y_train)\n",
        "y_pred = knn.predict(X_test)\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "s9bZbdxaDUAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "42. Train a KNN Regressor and analyze the effect of different distance metrics on prediction error.\n"
      ],
      "metadata": {
        "id": "2hoiYOEw_3x3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_regression\n",
        "\n",
        "X, y = make_regression(n_samples=500, n_features=5, noise=0.1)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
        "\n",
        "for p in [1, 2]:  # Manhattan and Euclidean\n",
        "    knr = KNeighborsRegressor(p=p)\n",
        "    knr.fit(X_train, y_train)\n",
        "    y_pred = knr.predict(X_test)\n",
        "    print(f\"p={p}, MSE={mean_squared_error(y_test, y_pred):.3f}\")\n"
      ],
      "metadata": {
        "id": "YC7rGPX_DWnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "43. Train a KNN Classifier and evaluate using ROC-AUC score.\n"
      ],
      "metadata": {
        "id": "YeDjkUPe_5qt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "y_score = knn.predict_proba(X_test)[:, 1]\n",
        "print(\"ROC AUC:\", roc_auc_score(y_test, y_score))\n"
      ],
      "metadata": {
        "id": "a8_MkEa9DZDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "44. Train a PCA model and visualize the variance captured by each principal component.\n"
      ],
      "metadata": {
        "id": "u_iCtxP2_8A-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA().fit(X)\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel(\"Number of Components\")\n",
        "plt.ylabel(\"Cumulative Explained Variance\")\n",
        "plt.title(\"Variance Explained by PCA\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1s5AOo3VDbZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "45. Train a KNN Classifier and perform feature selection before training.\n"
      ],
      "metadata": {
        "id": "cssihVy4_9jk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "selector = SelectKBest(score_func=f_classif, k=5)\n",
        "X_new = selector.fit_transform(X, y)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_new, y, random_state=42)\n",
        "\n",
        "knn.fit(X_train, y_train)\n",
        "print(\"Accuracy after feature selection:\", knn.score(X_test, y_test))\n"
      ],
      "metadata": {
        "id": "fo-sO2DbDe7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "46. Train a PCA model and visualize the data reconstruction error after reducing dimensions\n"
      ],
      "metadata": {
        "id": "ILEVA8U9__EH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=5)\n",
        "X_reduced = pca.fit_transform(X)\n",
        "X_reconstructed = pca.inverse_transform(X_reduced)\n",
        "reconstruction_error = np.mean((X - X_reconstructed)**2, axis=1)\n",
        "\n",
        "plt.hist(reconstruction_error, bins=30)\n",
        "plt.title(\"PCA Reconstruction Error\")\n",
        "plt.xlabel(\"Reconstruction Error\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "1VzvdXSlDigC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "47. Train a KNN Classifier and visualize the decision boundary.\n"
      ],
      "metadata": {
        "id": "d8-75zttAAqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.colors import ListedColormap\n",
        "\n",
        "X_vis, y_vis = make_classification(n_samples=300, n_features=2, n_redundant=0, random_state=42)\n",
        "knn.fit(X_vis, y_vis)\n",
        "\n",
        "h = .02\n",
        "x_min, x_max = X_vis[:, 0].min() - 1, X_vis[:, 0].max() + 1\n",
        "y_min, y_max = X_vis[:, 1].min() - 1, X_vis[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "Z = knn.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)\n",
        "\n",
        "plt.contourf(xx, yy, Z, cmap=ListedColormap(['#FFAAAA', '#AAFFAA']))\n",
        "plt.scatter(X_vis[:, 0], X_vis[:, 1], c=y_vis, edgecolor='k')\n",
        "plt.title(\"KNN Decision Boundary\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "O-Mcn3FIDn9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "48. Train a PCA model and analyze the effect of different numbers of components on data variance."
      ],
      "metadata": {
        "id": "aFhDEFl3ACSC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Load data\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Fit PCA without limiting components\n",
        "pca = PCA()\n",
        "pca.fit(X_scaled)\n",
        "\n",
        "# Explained variance ratio\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "cumulative_variance = np.cumsum(explained_variance)\n",
        "\n",
        "# Plot the explained variance\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(range(1, len(explained_variance)+1), cumulative_variance, marker='o', linestyle='--', color='b')\n",
        "plt.title('PCA - Cumulative Explained Variance')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Cumulative Explained Variance Ratio')\n",
        "plt.grid(True)\n",
        "plt.axhline(y=0.95, color='r', linestyle=':')  # optional line to show 95% threshold\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "BV6_T00YDq__"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}